{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AR6420/TwitterSentimentAnalysis/blob/main/Twitter_SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77bd6812",
      "metadata": {
        "id": "77bd6812"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option(\"display.max_rows\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb916af1",
      "metadata": {
        "id": "cb916af1"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('Ukraine_tweetys.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc4e9eb2",
      "metadata": {
        "id": "cc4e9eb2"
      },
      "outputs": [],
      "source": [
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "913f0461",
      "metadata": {
        "id": "913f0461"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b07851e",
      "metadata": {
        "id": "3b07851e"
      },
      "outputs": [],
      "source": [
        "# Find tweets in English only\n",
        "df_en = df[df['language']=='en']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "976f45a0",
      "metadata": {
        "id": "976f45a0"
      },
      "outputs": [],
      "source": [
        "df_en.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ca0d56a",
      "metadata": {
        "id": "0ca0d56a"
      },
      "outputs": [],
      "source": [
        "# Remove unnecessary columns\n",
        "df_en_clean = df_en.drop(columns = ['Tweet Id','Unnamed: 0','Unnamed: 0.1','verified'], axis  = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4f918e2",
      "metadata": {
        "id": "c4f918e2"
      },
      "outputs": [],
      "source": [
        "df_en_clean.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a4570ea",
      "metadata": {
        "id": "5a4570ea"
      },
      "outputs": [],
      "source": [
        "df_en_clean['Username'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2739f0e",
      "metadata": {
        "id": "c2739f0e"
      },
      "outputs": [],
      "source": [
        "df_en_clean['location'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeb075a8",
      "metadata": {
        "scrolled": true,
        "id": "aeb075a8"
      },
      "outputs": [],
      "source": [
        "df_en_clean['like count'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c4a6c96",
      "metadata": {
        "id": "9c4a6c96"
      },
      "outputs": [],
      "source": [
        "df_en_clean[df_en_clean['Username']=='StopVladdyDaddy']['Follower Count'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bd29f1a",
      "metadata": {
        "id": "4bd29f1a"
      },
      "outputs": [],
      "source": [
        "df_en_clean['Text'].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03698db3",
      "metadata": {
        "id": "03698db3"
      },
      "outputs": [],
      "source": [
        "df_en_clean.to_csv('cleaned_tweets.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20d38d8c",
      "metadata": {
        "id": "20d38d8c"
      },
      "source": [
        "## Tweets from Russia location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0863b69d",
      "metadata": {
        "id": "0863b69d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "position = 0\n",
        "positions = []\n",
        "for location in df_en_clean.location:\n",
        "    if type(location) != float:\n",
        "        location = location.lower().split()\n",
        "        if 'russia' in location and 'us' not in location and 'not' not in location and 'but' not in location:\n",
        "            positions.append(position)\n",
        "    else:\n",
        "        pass\n",
        "    position = position + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c7fa036",
      "metadata": {
        "id": "7c7fa036"
      },
      "outputs": [],
      "source": [
        "df_en_clean.iloc[positions[1],3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "989d5ec8",
      "metadata": {
        "id": "989d5ec8"
      },
      "outputs": [],
      "source": [
        "df_russia_en = pd.DataFrame()\n",
        "for position in positions:\n",
        "    entry = df_en_clean.iloc[position,:]\n",
        "    #print(row)\n",
        "    df_russia_en= pd.concat([df_russia_en,entry],axis = 1)\n",
        "    #row_2 = df.loc[1].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f64b427",
      "metadata": {
        "id": "9f64b427"
      },
      "outputs": [],
      "source": [
        "df_russia_en = df_russia_en.transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "786ab662",
      "metadata": {
        "id": "786ab662"
      },
      "outputs": [],
      "source": [
        "df_russia_en.location.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ad608af",
      "metadata": {
        "id": "3ad608af"
      },
      "outputs": [],
      "source": [
        "df_russia_en.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2968f152",
      "metadata": {
        "id": "2968f152"
      },
      "outputs": [],
      "source": [
        "df_russia_en.to_csv('Russia_loc.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b71f83",
      "metadata": {
        "id": "70b71f83"
      },
      "source": [
        "## Tweets from Ukraine location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2ff747b",
      "metadata": {
        "id": "f2ff747b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "position = 0\n",
        "positions = []\n",
        "for location in df_en_clean.location:\n",
        "    if type(location) != float:\n",
        "        location = location.lower().split()\n",
        "        if ('ukraine' in location and 'taxpayer' not in location and\\\n",
        "            'and' not in location and 'in' not in location\\\n",
        "            and 'us' not in location and 'stand' not in location\\\n",
        "            and 'is' not in location and 'free' not in location\\\n",
        "           and 'earth' not in location and 'uk' not in location\\\n",
        "            and 'with' not in location and 'germany' not in location\\\n",
        "           and 'ireland' not in location and 'to' not in location\\\n",
        "           and 'support' not in location and 'ca' not in location\\\n",
        "           and 'ny' not in location and 'all' not in location\\\n",
        "           and 'or' not in location and 'denmark' not in location):\n",
        "            positions.append(position)\n",
        "    else:\n",
        "        pass\n",
        "    position = position + 1\n",
        "print(df_en_clean.iloc[positions[0],:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e6e44c8",
      "metadata": {
        "id": "8e6e44c8"
      },
      "outputs": [],
      "source": [
        "df_ukraine_en = pd.DataFrame()\n",
        "for position in positions:\n",
        "    entry = df_en_clean.iloc[position,:]\n",
        "    #print(entry.location)\n",
        "    df_ukraine_en = pd.concat([df_ukraine_en,entry],axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f84154e",
      "metadata": {
        "id": "6f84154e"
      },
      "outputs": [],
      "source": [
        "df_ukraine_en = df_ukraine_en.transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a585b82d",
      "metadata": {
        "id": "a585b82d"
      },
      "outputs": [],
      "source": [
        "df_ukraine_en.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61ced281",
      "metadata": {
        "id": "61ced281"
      },
      "outputs": [],
      "source": [
        "df_ukraine_en.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85d242f5",
      "metadata": {
        "id": "85d242f5"
      },
      "outputs": [],
      "source": [
        "df_ukraine_en.location.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f451b18e",
      "metadata": {
        "id": "f451b18e"
      },
      "outputs": [],
      "source": [
        "df_ukraine_en.to_csv('Ukraine_loc.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77c8d18a",
      "metadata": {
        "id": "77c8d18a"
      },
      "source": [
        "## Tweets from all other location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad0b6d81",
      "metadata": {
        "id": "ad0b6d81"
      },
      "outputs": [],
      "source": [
        "df_other_en = pd.concat([df_en_clean, df_ukraine_en, df_russia_en]).drop_duplicates(keep=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "856068a0",
      "metadata": {
        "id": "856068a0"
      },
      "outputs": [],
      "source": [
        "df_other_en.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7c0f0fc",
      "metadata": {
        "id": "f7c0f0fc"
      },
      "outputs": [],
      "source": [
        "df_other_en.to_csv('Other_loc.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35f78bae",
      "metadata": {
        "id": "35f78bae"
      },
      "source": [
        "## High impact tweets (like, retweet and reply > 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b75d3a86",
      "metadata": {
        "id": "b75d3a86"
      },
      "outputs": [],
      "source": [
        "df_en_highimpact = df_en_clean[df_en_clean['like count'] > 1]\n",
        "df_en_highimpact = df_en_highimpact[df_en_highimpact['retweet count']>1]\n",
        "df_en_highimpact = df_en_highimpact[df_en_highimpact['reply count']>1]\n",
        "df_en_highimpact.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c07fe1a4",
      "metadata": {
        "id": "c07fe1a4"
      },
      "outputs": [],
      "source": [
        "df_en_highimpact.to_csv('highimpact.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49d2eddb",
      "metadata": {
        "id": "49d2eddb"
      },
      "source": [
        "## Topic modeling for high impact tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2d5a511",
      "metadata": {
        "id": "f2d5a511"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "import spacy\n",
        "import gensim.corpora as corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d43844c",
      "metadata": {
        "id": "8d43844c"
      },
      "outputs": [],
      "source": [
        "# clean_text takes in a pd.Series and removes special characters, stopwords, etc\n",
        "def clean_text_nltk(pd_column):\n",
        "    stop_words = list(set(stopwords.words('english')))  #use nltk stopwords\n",
        "    clean = []\n",
        "    for i in pd_column:\n",
        "        i = i.lower()  #turn to lower case\n",
        "        #print(i)\n",
        "\n",
        "        #preprocess to split the words\n",
        "        #re.split() has problem with '-' if no space between words, so first do i.split('-')\n",
        "        i = i.split('-')\n",
        "        i = ' '.join(i)\n",
        "        i = i.split()\n",
        "        #print(i)\n",
        "\n",
        "        split_sent2 = []\n",
        "\n",
        "        #remove stopwords and special characters\n",
        "        for word in i:\n",
        "\n",
        "            new_word = []\n",
        "            if word not in stop_words and word[0]!='@' and word[0:4] != 'http' and word[0]!='#':\n",
        "            #check each word to remove stop words, @username, website, hash-tag\n",
        "\n",
        "                for letter in word:  #check each letter in word to remove special characters\n",
        "                    if letter.isalpha(): #or letter.isdigit():\n",
        "                        new_word.append(letter)\n",
        "                    else:\n",
        "                        pass\n",
        "                word2 = ''.join(new_word)\n",
        "                #print(word2)\n",
        "\n",
        "                if word2 in stop_words or len(word2) < 3: #check stopwords again\n",
        "                    pass\n",
        "                else:\n",
        "                    split_sent2.append(word2)\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "        #print(split_sent2)\n",
        "\n",
        "        sent2 = ' '.join(split_sent2)\n",
        "        clean.append(sent2)\n",
        "\n",
        "    #print(clean[0:10])\n",
        "    return(clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68f1d368",
      "metadata": {
        "id": "68f1d368"
      },
      "outputs": [],
      "source": [
        "test = clean_text_nltk(df_en_highimpact['Text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62865f0e",
      "metadata": {
        "id": "62865f0e"
      },
      "outputs": [],
      "source": [
        "test[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4f96e1b",
      "metadata": {
        "id": "e4f96e1b"
      },
      "outputs": [],
      "source": [
        "# Tokenization, input is pd.Series with full sentences\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "def nltk_tokenizer(pd_column):\n",
        "    tokenized = []\n",
        "    for i in pd_column:\n",
        "        word_tokens = word_tokenize(i)\n",
        "        tokenized.append(word_tokens)\n",
        "    return tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05da8833",
      "metadata": {
        "id": "05da8833"
      },
      "outputs": [],
      "source": [
        "df_tokenized = pd.Series(nltk_tokenizer(test))\n",
        "df_tokenized.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43ceae37",
      "metadata": {
        "id": "43ceae37"
      },
      "outputs": [],
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(df_tokenized, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[df_tokenized], threshold=100)\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[df_tokenized[0]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68c3b1cb",
      "metadata": {
        "id": "68c3b1cb"
      },
      "outputs": [],
      "source": [
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "841fd363",
      "metadata": {
        "id": "841fd363"
      },
      "outputs": [],
      "source": [
        "data_words_bigrams = make_bigrams(df_tokenized)\n",
        "print(data_words_bigrams[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "589d799d",
      "metadata": {
        "id": "589d799d"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out\n",
        "\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "#print(data_lemmatized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ddc80e3",
      "metadata": {
        "id": "7ddc80e3"
      },
      "outputs": [],
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "# View\n",
        "print(corpus[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f067c930",
      "metadata": {
        "id": "f067c930"
      },
      "outputs": [],
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=5,\n",
        "                                           random_state=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f1de4d",
      "metadata": {
        "id": "65f1de4d"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c228ebb",
      "metadata": {
        "id": "3c228ebb"
      },
      "outputs": [],
      "source": [
        "lda_model2 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=10,\n",
        "                                           random_state=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b065daf4",
      "metadata": {
        "scrolled": true,
        "id": "b065daf4"
      },
      "outputs": [],
      "source": [
        "pprint(lda_model2.print_topics())\n",
        "doc_lda2 = lda_model2[corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75f140ed",
      "metadata": {
        "id": "75f140ed"
      },
      "outputs": [],
      "source": [
        "lda_model3 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=7,\n",
        "                                           random_state=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c7a44d6",
      "metadata": {
        "id": "1c7a44d6"
      },
      "outputs": [],
      "source": [
        "pprint(lda_model3.print_topics())\n",
        "doc_lda2 = lda_model2[corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab7c791b",
      "metadata": {
        "id": "ab7c791b"
      },
      "outputs": [],
      "source": [
        "pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3b8260c",
      "metadata": {
        "scrolled": true,
        "id": "a3b8260c"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
        "vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "307dc796",
      "metadata": {
        "id": "307dc796"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_model2, corpus, id2word)\n",
        "vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65b37b6c",
      "metadata": {
        "id": "65b37b6c"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_model3, corpus, id2word)\n",
        "vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8cd8d6d",
      "metadata": {
        "id": "f8cd8d6d"
      },
      "outputs": [],
      "source": [
        "lemmatized_tweets = []\n",
        "for ls in data_lemmatized:\n",
        "    tweet = ' '.join(ls)\n",
        "    lemmatized_tweets.append(tweet)\n",
        "\n",
        "lemmatized_tweets[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba64f95d",
      "metadata": {
        "id": "ba64f95d"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "dtm = tfidf.fit_transform(lemmatized_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2563fe1",
      "metadata": {
        "id": "d2563fe1"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import NMF\n",
        "nmf_model = NMF(n_components=5,random_state=42) #choosing 5 topics\n",
        "nmf_model.fit(dtm)\n",
        "tfidf.get_feature_names_out()[2400]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bb77fd1",
      "metadata": {
        "id": "7bb77fd1"
      },
      "outputs": [],
      "source": [
        "topic_lists = []\n",
        "for index,topic in enumerate(nmf_model.components_):\n",
        "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
        "    topic_list=[tfidf.get_feature_names_out()[i] for i in topic.argsort()[-30:]]\n",
        "    print([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-30:]])\n",
        "    topic_lists.append(topic_list)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc682cab",
      "metadata": {
        "id": "fc682cab"
      },
      "outputs": [],
      "source": [
        "topic_list1 = topic_lists[0]\n",
        "topic_list2 = topic_lists[1]\n",
        "topic_list3 = topic_lists[2]\n",
        "topic_list4 = topic_lists[3]\n",
        "topic_list5 = topic_lists[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4ceb293",
      "metadata": {
        "id": "e4ceb293"
      },
      "outputs": [],
      "source": [
        "set(topic_list1)&set(topic_list2)&set(topic_list3)&set(topic_list4)&set(topic_list5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8ab79ff",
      "metadata": {
        "id": "b8ab79ff"
      },
      "outputs": [],
      "source": [
        "topic_results = nmf_model.transform(dtm)\n",
        "df_en_highimpact['Topic'] = topic_results.argmax(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbbf282c",
      "metadata": {
        "id": "bbbf282c"
      },
      "outputs": [],
      "source": [
        "df_en_highimpact.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61869b1b",
      "metadata": {
        "id": "61869b1b"
      },
      "outputs": [],
      "source": [
        "mytopic_dict = {0:'War cause',1:'Millitary progress',2:'Refugee and human rights',\n",
        "                3:'Economic impact of the war',4:'News and journalism in war'}\n",
        "df_en_highimpact['Topic Label'] = df_en_highimpact['Topic'].map(mytopic_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9593b252",
      "metadata": {
        "id": "9593b252"
      },
      "outputs": [],
      "source": [
        "df_en_highimpact.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc379221",
      "metadata": {
        "id": "cc379221"
      },
      "outputs": [],
      "source": [
        "df_en_highimpact['Topic'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e80e0d4b",
      "metadata": {
        "id": "e80e0d4b"
      },
      "outputs": [],
      "source": [
        "df_en_highimpact.to_csv('highimpact_nmf.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27220f0f",
      "metadata": {
        "id": "27220f0f"
      },
      "source": [
        "## Sentiment analysis for high impact tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4c21bfd",
      "metadata": {
        "scrolled": true,
        "id": "d4c21bfd"
      },
      "outputs": [],
      "source": [
        "# Import SentimentIntensityAnalyzer and create an sid object\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "sid = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86bc0b84",
      "metadata": {
        "id": "86bc0b84"
      },
      "outputs": [],
      "source": [
        "def sentiment_rating(string):\n",
        "    sid_dict = sid.polarity_scores(string)\n",
        "    if sid_dict['compound']>= 0.33:\n",
        "        return ('Positive')\n",
        "    elif sid_dict['compound']<= -0.33:\n",
        "        return ('Negative')\n",
        "    elif sid_dict['compound'] > -0.33 and sid_dict['compound'] < 0.33:\n",
        "        return('Neutral')\n",
        "    else:\n",
        "        return('Score out of range')\n",
        "\n",
        "sentiment_nltk = []\n",
        "for tweet in df_en_highimpact['Text']:\n",
        "    sid_dict = sentiment_rating(tweet)\n",
        "    sentiment_nltk.append(sid_dict)\n",
        "\n",
        "df_en_highimpact['sentiment_nltk'] = sentiment_nltk\n",
        "#df_en_highimpact['sentiment_rating'] = sentimen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b7aa6a",
      "metadata": {
        "id": "08b7aa6a"
      },
      "outputs": [],
      "source": [
        "df_en_highimpact['sentiment_nltk'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88357f0e",
      "metadata": {
        "id": "88357f0e"
      },
      "source": [
        "## Sentiment analysis for Russia location tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0056a5e",
      "metadata": {
        "id": "d0056a5e"
      },
      "outputs": [],
      "source": [
        "def sentiment_rating(string):\n",
        "    sid_dict = sid.polarity_scores(string)\n",
        "    if sid_dict['compound']>= 0.33:\n",
        "        return ('Positive')\n",
        "    elif sid_dict['compound']<= -0.33:\n",
        "        return ('Negative')\n",
        "    elif sid_dict['compound'] > -0.33 and sid_dict['compound'] < 0.33:\n",
        "        return('Neutral')\n",
        "    else:\n",
        "        return('Score out of range')\n",
        "\n",
        "sentiment_nltk = []\n",
        "for tweet in df_russia_en['Text']:\n",
        "    sid_dict = sentiment_rating(tweet)\n",
        "    sentiment_nltk.append(sid_dict)\n",
        "\n",
        "\n",
        "df_russia_en['sentiment_nltk'] = sentiment_nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d603786d",
      "metadata": {
        "id": "d603786d"
      },
      "outputs": [],
      "source": [
        "df_russia_en.sentiment_nltk.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adb7b069",
      "metadata": {
        "id": "adb7b069"
      },
      "outputs": [],
      "source": [
        "sentiment_nltk = []\n",
        "for tweet in df_russia_en['Text']:\n",
        "    sid_dict = sentiment_rating(tweet)\n",
        "    sentiment_nltk.append(sid_dict)\n",
        "\n",
        "df_russia_en['sentiment_rating'] = sentiment_nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f025345",
      "metadata": {
        "scrolled": true,
        "id": "0f025345"
      },
      "outputs": [],
      "source": [
        "df_russia_en.sentiment_rating.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6479f751",
      "metadata": {
        "id": "6479f751"
      },
      "outputs": [],
      "source": [
        "df_russia_en[df_russia_en['sentiment_nltk'] == 'Positive'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "600234d2",
      "metadata": {
        "id": "600234d2"
      },
      "outputs": [],
      "source": [
        "df_russia_en[df_russia_en['sentiment_nltk'] == 'Neutral'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b512c69f",
      "metadata": {
        "id": "b512c69f"
      },
      "outputs": [],
      "source": [
        "df_russia_en[df_russia_en['sentiment_nltk'] == 'Negative'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4de087bf",
      "metadata": {
        "id": "4de087bf"
      },
      "source": [
        "## Sentiment analysis for Ukraine location tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb55e31b",
      "metadata": {
        "id": "cb55e31b"
      },
      "outputs": [],
      "source": [
        "def sentiment_rating(string):\n",
        "    sid_dict = sid.polarity_scores(string)\n",
        "    if sid_dict['compound']>= 0.33:\n",
        "        return ('Positive')\n",
        "    elif sid_dict['compound']<= -0.33:\n",
        "        return ('Negative')\n",
        "    elif sid_dict['compound'] > -0.33 and sid_dict['compound'] < 0.33:\n",
        "        return('Neutral')\n",
        "    else:\n",
        "        return('Score out of range')\n",
        "\n",
        "sentiment_nltk = []\n",
        "for tweet in df_ukraine_en['Text']:\n",
        "    sid_dict = sentiment_rating(tweet)\n",
        "    sentiment_nltk.append(sid_dict)\n",
        "\n",
        "\n",
        "df_ukraine_en['sentiment_nltk'] = sentiment_nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d6fc08",
      "metadata": {
        "id": "94d6fc08"
      },
      "outputs": [],
      "source": [
        "df_ukraine_en.sentiment_nltk.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8152d7aa",
      "metadata": {
        "id": "8152d7aa"
      },
      "outputs": [],
      "source": [
        "df_ukraine_en[df_ukraine_en['sentiment_nltk'] == 'Positive'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a70c2116",
      "metadata": {
        "id": "a70c2116"
      },
      "outputs": [],
      "source": [
        "df_ukraine_en[df_ukraine_en['sentiment_nltk'] == 'Neutral'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac4fa76",
      "metadata": {
        "id": "6ac4fa76"
      },
      "outputs": [],
      "source": [
        "df_ukraine_en[df_ukraine_en['sentiment_nltk'] == 'Negative'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf9bffdc",
      "metadata": {
        "id": "bf9bffdc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}